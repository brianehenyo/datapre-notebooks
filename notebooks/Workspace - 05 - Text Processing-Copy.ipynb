{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nutritional-species",
   "metadata": {},
   "source": [
    "# Hands-on: Text Processing\n",
    "This hands-on will cover the necessary steps in a text processing pipeline for Human Language Technologies (HLT). Some examples of projects and tasks in which this pipeline will be useful are the following:\n",
    "- **Language Translation** - translation of a sentence or body of text from one language to another\n",
    "- **Word Sense Disambiguation** - determining the meaning and context of a polysemic word in a body of text\n",
    "- **Sentiment Analysis** - determining the overall sentiment towards a certain topic or word, whether it's positive, negative, or neutral\n",
    "- **Topic Modeling** - identifying the different topics discusses in a text and determining the most prevalent one\n",
    "\n",
    "And there are others more like question answering, information extraction, and more recently, detecting mis/disinformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-brother",
   "metadata": {},
   "source": [
    "## Pre-processing Pipeline\n",
    "\n",
    "- **Tokenization** — split sentences into words and symbols\n",
    "- **Convert to lowercase**\n",
    "- **Removing unnecessary punctuation, tags, and emojis**\n",
    "- **Removing stop words** — removing frequently occurring words like articles (e.g. ”the”, ”is”, etc.) that do not have specific meanings\n",
    "- **Stemming** — transforms a word to their root form by removing inflectional endings. It is done by usually dropping the suffixes.\n",
    "\n",
    "```\n",
    "The stemmed form of cries is: cri\n",
    "The stemmed form of crying is: cry\n",
    "```\n",
    "\n",
    "- **Lemmatization** — properly removing inflectional endings by determining the part of speech and doing morphological analysis. It transforms words to their base or dictionary form.\n",
    "\n",
    "```\n",
    "The lemmatized form of cries is: cry\n",
    "The lemmatized form of crying is: cry\n",
    "```\n",
    "\n",
    "> **NOTE:** Not all HLT tasks/projects will follow the same pipeline. For example, topic modeling were proven to be better with stop words, so the removal of stop words is typicallly skipped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "urban-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-equilibrium",
   "metadata": {},
   "source": [
    "### Load JSON dataset containing posts from r/waze\n",
    "This dataset was collected in 2019 using the pushshift.io API. It contains the `submission ID` and the post's `body` of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "creative-purse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "offshore-nickname",
   "metadata": {},
   "source": [
    "### Remove URLs from text\n",
    "You can use [Regex101](https://regex101.com/) for checking your regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annual-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeURLFromText(text):\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-robertson",
   "metadata": {},
   "source": [
    "### Pre-process post's body\n",
    "`gensim`'s `simple_preprocess()` converts a text into a list of tokens that are already in lowercase.\n",
    "\n",
    "This step also removes stop words and words with less than 3 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coordinate-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-stability",
   "metadata": {},
   "source": [
    "### Stem and lemmatize per text\n",
    "Lemmatize the word first. If there will be words missed, the stemmer should be able to handle it. The lemmatization will only be done for verbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-mileage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "descending-frame",
   "metadata": {},
   "source": [
    "### Create a `gensim` Dictionary\n",
    "This will organize your bag of words into word <-> id mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "coated-infection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "romantic-anger",
   "metadata": {},
   "source": [
    "### Filter words\n",
    "Filter out tokens that appear in\n",
    "\n",
    "- less than `no_below` documents (absolute number) or\n",
    "- more than `no_above` documents (fraction of total corpus size, not absolute number).\n",
    "- after (1) and (2), keep only the first `keep_n` most frequent tokens (or keep all if `None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "scheduled-possession",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "single-schema",
   "metadata": {},
   "source": [
    "### Map bag of words per document\n",
    "\n",
    "So far, we have only counted the occurrence of each word across all documents. Next, we need to know how often each word appeared in each document, but now using the IDs generated in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fleet-hobby",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bigger-fundamental",
   "metadata": {},
   "source": [
    "### Compute the TF-IDF per word in a document\n",
    "\n",
    "- **Term Frequency (TF)** is the number of times token `t` appears in a document divided by the total number of tokens in the document.\n",
    "- **Inverse Document Frequency (IDF)** is the log(N/n), where, `N` is the number of documents and `n` is the number of documents a token t has appeared in. A less frequently used word will have a high IDF, whereas the IDF of a frequent word is likely to be low. \n",
    "\n",
    "We calculate TF-IDF value of a term as = **TF * IDF**\n",
    "\n",
    "Example:\n",
    "```\n",
    "Document 1: \"I worked my whole life, just to get right, just to be like\"\n",
    "[work, 1]\n",
    "[whole, 1]\n",
    "[life, 1]\n",
    "[just, 2]\n",
    "[right, 1]\n",
    "[like, 1]\n",
    "```\n",
    "\n",
    "```\n",
    "Document 2: \"I worked my whole life, just to get high, just to realize\"\n",
    "[work, 1]\n",
    "[whole, 1]\n",
    "[life, 1]\n",
    "[just, 2]\n",
    "[high, 1]\n",
    "[realize, 1]\n",
    "```\n",
    "\n",
    "```\n",
    "TF('just',Document1) = 2/7, IDF('just')=log(2/2) = 0\n",
    "TF('right',Document1) = 1/7,  IDF(‘right’)=log(2/1) = 0.30\n",
    "\n",
    "TF-IDF(‘just’, Document1) = (2/7)*0 = 0\n",
    "TF-IDF(‘right’, Document1) = (1/7)*0.30 = 0.42\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "entitled-midwest",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
